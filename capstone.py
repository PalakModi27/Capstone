# -*- coding: utf-8 -*-
"""Capstone

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wC-GJinbeWUFPllVsoglOXpQuOJeiynC
"""

pip install transformers

"""The library downloads pretrained models for Natural Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG), such as completing a prompt with new text or translating in another language."""

from transformers import pipeline
classifier = pipeline('sentiment-analysis')
#When typing this command for the first time, a pretrained model and its tokenizer are downloaded and cached.
#tokenizer's job is to preprocess the text for the model, which is then responsible for making predictions.
#The pipeline groups all of that together, and post-process the predictions to make them readable.

classifier('Mumbai is the best party place.')

classifier('It became very difficult for me to breathe in Delhi.')

results = classifier(["It was amazing visiting Shimla.",
           "The scenery was beautiful.",
           "I had trouble breathing though."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")

"""By default, the model downloaded for this pipeline is called "distilbert-base-uncased-finetuned-sst-2-english"
"""

classifier = pipeline('sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")

classifier = pipeline('sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")
#This model works in english, french, dutch, etc.

classifier("Esperamos que no lo odie.")
#We hope you don't hate it
#neutral scores have been provided

#Using tokenizer
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
# This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.
model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)

classifier("Kolkata is famous for it's sweets")

classifier("Kolkata is a crowded place.")

#model implementation
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

inputs = tokenizer("Ladakh is the best place for serenity and be free from mental stress.")
#returns ids of tokens
#returns attention masks of each word

print(inputs)

tf_batch = tokenizer(
    ["We are very happy to visit Manali.", "We hope you enjoyed the toy train."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf"
)
#padding to make all sentences of same length

for key, value in tf_batch.items():
    print(f"{key}: {value.numpy().tolist()}")

tf_outputs = tf_model(tf_batch)

print(tf_outputs)

#softmax to get predictions
import tensorflow as tf
tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)

print(tf_predictions)

import tensorflow as tf
tf_outputs = tf_model(tf_batch, labels = tf.constant([1, 0]))
#finetuning - weights of a trained network as the starting values for a new network

#tokenizer.save_pretrained(save_directory)
#model.save_pretrained(save_directory)
#saving the tokenizer

from transformers import pipeline

classifier = pipeline(task="ner")
preds = classifier("Hugging Face is a French company based in New York City.")
preds = [
    {
        "entity": pred["entity"],
        "score": round(pred["score"], 4),
        "index": pred["index"],
        "word": pred["word"],
        "start": pred["start"],
        "end": pred["end"],
    }
    for pred in preds
]
print(*preds, sep="\n")